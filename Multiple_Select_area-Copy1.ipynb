{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Dbi6hPv71ND1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "#from tkinter import Tk, filedialog\n",
    "from skimage import img_as_ubyte\n",
    "from skimage.segmentation import clear_border\n",
    "from skimage import measure\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import normalize\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import pyvips\n",
    "import re\n",
    "import statistics\n",
    "from tqdm import tqdm\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel('ERROR')\n",
    "# Image sizes need to be divisible by 16\n",
    "IMG_HEIGHT = 688\n",
    "IMG_WIDTH  = 432\n",
    "coordinates = (300, 300, 740, 1000)  # Specify the ROI coordinates (left, upper, right, lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 159813,
     "status": "ok",
     "timestamp": 1687204807973,
     "user": {
      "displayName": "Philipp W.",
      "userId": "16506019178879371982"
     },
     "user_tz": -120
    },
    "id": "yexyFmiVeVYE",
    "outputId": "ca77e093-e829-448a-da67-c9b8e7573296"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': 'Scan070602_00000000_0000.png', 'path': 'documents/Scan070602_2/Scan070602/', 'angPos': 904, 'projPerAng': 64}\n"
     ]
    }
   ],
   "source": [
    "# Colab import\n",
    "file_path = \"documents/Scan070602_2/Scan070602/Scan070602_00000000_0000.png\"\n",
    "\n",
    "file_dir, file_name = os.path.split(file_path)\n",
    "\n",
    "name_numberAngPos = file_name.split('_0')[0] + \"_0\"\n",
    "\n",
    "# Determination of the number of images per angular position\n",
    "numberAngPos = len([f for f in os.listdir(file_dir) if f.startswith(name_numberAngPos) and f.endswith(\"_0000.png\")]) - 1\n",
    "\n",
    "# Determination of the total number of projection images\n",
    "numberProjIm = len([f for f in os.listdir(file_dir) if f.startswith(name_numberAngPos + \"0000000_\") and f.endswith(\".png\")]) - 1\n",
    "\n",
    "\n",
    "# Define Image Data List with Keywords\n",
    "# The entire image is considered the ROI\n",
    "imageData = {\n",
    "    'filename': file_name,\n",
    "    'path': file_dir + \"/\",\n",
    "    'angPos': numberAngPos,\n",
    "    'projPerAng': numberProjIm,\n",
    "}\n",
    "\n",
    "\n",
    "#return imageData, ImageROI\n",
    "print(imageData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "pnWDwDatcFR3"
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_image_filenames(max_num1, max_num2):\n",
    "    for num1 in range(max_num1 + 1):\n",
    "        for num2 in range(max_num2+ 1):\n",
    "            filename = f\"Scan070602_{num1:08d}_{num2:04d}.png\"\n",
    "            filenames.append(filename)\n",
    "    return filenames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "c0FdzsqxHTFi"
   },
   "outputs": [],
   "source": [
    "def load_batch(image_paths):\n",
    "    images = []\n",
    "    for image_path in image_paths:\n",
    "        image = pyvips.Image.new_from_file(image_path, access='sequential')\n",
    "        memory_buffer = image.write_to_memory()\n",
    "        # Convert the memory buffer to a NumPy array\n",
    "        height, width = image.height, image.width\n",
    "        np_image = np.frombuffer(memory_buffer, dtype=np.uint16).reshape(height, width)\n",
    "        images.append(np_image)\n",
    "        isinstance(np_image, np.ndarray)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "R8aXcloQlYn1"
   },
   "outputs": [],
   "source": [
    "def preprocess_images(batch_images):\n",
    "    preprocessed_images = []\n",
    "    for image in batch_images:\n",
    "        image = image[coordinates[1]:coordinates[3], coordinates[0]:coordinates[2]]\n",
    "        # Resize the image using OpenCV\n",
    "        resized_image = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))\n",
    "        # Append to the list of preprocessed images\n",
    "        preprocessed_images.append(resized_image)\n",
    "    #maybe change to -1\n",
    "    preprocessed_images = np.expand_dims(normalize(np.array(preprocessed_images), axis=1),3)\n",
    "    print(np.shape(preprocessed_images))\n",
    "    return preprocessed_images\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "C0PjmlpCZv7v"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n",
    "\n",
    "################################################################\n",
    "def simple_unet_model(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS):\n",
    "#Build the model\n",
    "    inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "    #s = Lambda(lambda x: x / 255)(inputs)   #No need for this if inputs are normalized beforehand\n",
    "    s = inputs\n",
    "\n",
    "    #Contraction path\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)\n",
    "    c1 = Dropout(0.1)(c1)\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
    "    c2 = Dropout(0.1)(c2)\n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
    "    c3 = Dropout(0.2)(c3)\n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
    "    c4 = Dropout(0.2)(c4)\n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
    "    p4 = MaxPooling2D(pool_size=(2, 2))(c4)\n",
    "\n",
    "    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
    "    c5 = Dropout(0.3)(c5)\n",
    "    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
    "\n",
    "    #Expansive path\n",
    "    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n",
    "    c6 = Dropout(0.2)(c6)\n",
    "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
    "\n",
    "    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
    "    c7 = Dropout(0.2)(c7)\n",
    "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
    "\n",
    "    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
    "    c8 = Dropout(0.1)(c8)\n",
    "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
    "\n",
    "    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "    u9 = concatenate([u9, c1], axis=3)\n",
    "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
    "    c9 = Dropout(0.1)(c9)\n",
    "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
    "\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "530kgHg6xhkM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 688, 432, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 688, 432, 16  160         ['input_2[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 688, 432, 16  0           ['conv2d_19[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 688, 432, 16  2320        ['dropout_9[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPooling2D)  (None, 344, 216, 16  0          ['conv2d_20[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 344, 216, 32  4640        ['max_pooling2d_4[0][0]']        \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 344, 216, 32  0           ['conv2d_21[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 344, 216, 32  9248        ['dropout_10[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPooling2D)  (None, 172, 108, 32  0          ['conv2d_22[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 172, 108, 64  18496       ['max_pooling2d_5[0][0]']        \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 172, 108, 64  0           ['conv2d_23[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 172, 108, 64  36928       ['dropout_11[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPooling2D)  (None, 86, 54, 64)  0           ['conv2d_24[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 86, 54, 128)  73856       ['max_pooling2d_6[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 86, 54, 128)  0           ['conv2d_25[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 86, 54, 128)  147584      ['dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPooling2D)  (None, 43, 27, 128)  0          ['conv2d_26[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 43, 27, 256)  295168      ['max_pooling2d_7[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 43, 27, 256)  0           ['conv2d_27[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 43, 27, 256)  590080      ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_transpose_4 (Conv2DTran  (None, 86, 54, 128)  131200     ['conv2d_28[0][0]']              \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 86, 54, 256)  0           ['conv2d_transpose_4[0][0]',     \n",
      "                                                                  'conv2d_26[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 86, 54, 128)  295040      ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 86, 54, 128)  0           ['conv2d_29[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 86, 54, 128)  147584      ['dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_transpose_5 (Conv2DTran  (None, 172, 108, 64  32832      ['conv2d_30[0][0]']              \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 172, 108, 12  0           ['conv2d_transpose_5[0][0]',     \n",
      "                                8)                                'conv2d_24[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 172, 108, 64  73792       ['concatenate_5[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 172, 108, 64  0           ['conv2d_31[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 172, 108, 64  36928       ['dropout_15[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_transpose_6 (Conv2DTran  (None, 344, 216, 32  8224       ['conv2d_32[0][0]']              \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 344, 216, 64  0           ['conv2d_transpose_6[0][0]',     \n",
      "                                )                                 'conv2d_22[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 344, 216, 32  18464       ['concatenate_6[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 344, 216, 32  0           ['conv2d_33[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 344, 216, 32  9248        ['dropout_16[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_transpose_7 (Conv2DTran  (None, 688, 432, 16  2064       ['conv2d_34[0][0]']              \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 688, 432, 32  0           ['conv2d_transpose_7[0][0]',     \n",
      "                                )                                 'conv2d_20[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 688, 432, 16  4624        ['concatenate_7[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 688, 432, 16  0           ['conv2d_35[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 688, 432, 16  2320        ['dropout_17[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 688, 432, 1)  17          ['conv2d_36[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,940,817\n",
      "Trainable params: 1,940,817\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "IMG_CHANNELS = 1\n",
    "\n",
    "def get_model():\n",
    "    return simple_unet_model(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
    "model = get_model()\n",
    "model.load_weights('documents/dropletonly.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pWvqaTkgavRI"
   },
   "outputs": [],
   "source": [
    "def post_processing(predictions):\n",
    "    # Post-processing\n",
    "    processed_images = []\n",
    "    for prediction in predictions:\n",
    "        # Binarize\n",
    "        categoryImg_water = (prediction >= 0.3).astype(np.uint8)\n",
    "\n",
    "        # Calculate buffer size (e.g., min dimension divided by 10)\n",
    "        #buffer_size = min(categoryImg_water.shape) // 10\n",
    "        #print(buffer_size)\n",
    "\n",
    "        # Despeckling and clearing border\n",
    "        #categoryImg_water_desp = clear_border(categoryImg_water, buffer_size=buffer_size)\n",
    "        categoryImg_water_desp = categoryImg_water\n",
    "        # Display despeckled image\n",
    "\n",
    "        # Morphological operations\n",
    "        se = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (20, 20))\n",
    "        categoryImg_water_morph = cv2.morphologyEx(categoryImg_water_desp.astype(np.uint8), cv2.MORPH_OPEN, se)\n",
    "        categoryImg_water_morph = cv2.morphologyEx(categoryImg_water_morph, cv2.MORPH_CLOSE, se)\n",
    "\n",
    "\n",
    "        # Append to processed_images list\n",
    "        processed_images.append(categoryImg_water)\n",
    "\n",
    "    return processed_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_phpskfhm43y"
   },
   "outputs": [],
   "source": [
    "def get_regionprops(processed_images, min_area=600):\n",
    "\n",
    "    for processed_image in processed_images:\n",
    "        # Label the connected components\n",
    "        labeled_droplets, _ = measure.label(processed_image, return_num=True, connectivity=2)\n",
    "        # Find region properties\n",
    "        regions = measure.regionprops(labeled_droplets)\n",
    "\n",
    "        # Filter regions based on area\n",
    "        filtered_regions = [region for region in regions if region.area >= min_area]\n",
    "\n",
    "        # Find the lowest droplet among filtered regions\n",
    "        if filtered_regions:\n",
    "            # Select the droplet with the maximum vertical position (row component of centroid)\n",
    "            lowest_droplet = max(filtered_regions, key=lambda region: region.centroid[0])\n",
    "\n",
    "            # Create a binary image with just the lowest droplet\n",
    "            test_img = np.zeros_like(processed_image, dtype=np.uint8)\n",
    "            for coord in lowest_droplet.coords:\n",
    "                test_img[coord[0], coord[1]] = 1\n",
    "            lowest_droplet_props = {\n",
    "                'centroid': lowest_droplet.centroid,\n",
    "                'area': lowest_droplet.area,\n",
    "                'bbox': lowest_droplet.bbox,\n",
    "                'filled_area': lowest_droplet.filled_area\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            # Set default values if no droplet meets the criteria\n",
    "            lowest_droplet_props = {\n",
    "                'centroid': (0, 0),\n",
    "                'area': 0,\n",
    "                'bbox': (0, 0, 0, 0),\n",
    "                'filled_area': 0\n",
    "            }\n",
    "        props[len(props) + 1] = lowest_droplet_props\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "4GtGYtKDidJk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/3676 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 688, 432, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-21 04:52:07.822155: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [16,688,432,1]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "Processing batches:   0%|          | 1/3676 [00:01<1:19:16,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 688, 432, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-21 04:52:09.114100: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [16,688,432,1]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "Processing batches:   0%|          | 2/3676 [00:02<1:17:19,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 688, 432, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-21 04:52:10.347704: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [16,688,432,1]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "Processing batches:   0%|          | 3/3676 [00:03<1:16:21,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 688, 432, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-21 04:52:11.580654: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [16,688,432,1]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "Processing batches:   0%|          | 4/3676 [00:04<1:15:28,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 688, 432, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-21 04:52:12.800367: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [16,688,432,1]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "Processing batches:   0%|          | 5/3676 [00:06<1:15:45,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 688, 432, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-21 04:52:14.036508: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [16,688,432,1]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "Processing batches:   0%|          | 6/3676 [00:07<1:16:23,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 688, 432, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-21 04:52:15.302114: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [16,688,432,1]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "Processing batches:   0%|          | 7/3676 [00:08<1:15:11,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 688, 432, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-21 04:52:16.496844: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [16,688,432,1]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "Processing batches:   0%|          | 8/3676 [00:09<1:15:37,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 688, 432, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-21 04:52:17.737007: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [16,688,432,1]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "Processing batches:   0%|          | 9/3676 [00:11<1:14:57,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 688, 432, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-21 04:52:18.958238: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [16,688,432,1]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "Processing batches:   0%|          | 10/3676 [00:12<1:15:32,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 688, 432, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-21 04:52:20.200107: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [16,688,432,1]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "Processing batches:   0%|          | 11/3676 [00:13<1:15:05,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 688, 432, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-21 04:52:21.433689: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [16,688,432,1]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "Processing batches:   0%|          | 12/3676 [00:15<1:19:27,  1.30s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m batch_image_paths \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(imageData[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m], filename) \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m batch_filenames]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Load the batch\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m batch_images \u001b[38;5;241m=\u001b[39m \u001b[43mload_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_image_paths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Preprocess the batch\u001b[39;00m\n\u001b[1;32m     17\u001b[0m preprocessed_batch_images \u001b[38;5;241m=\u001b[39m preprocess_images(batch_images)\n",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m, in \u001b[0;36mload_batch\u001b[0;34m(image_paths)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_path \u001b[38;5;129;01min\u001b[39;00m image_paths:\n\u001b[1;32m      4\u001b[0m     image \u001b[38;5;241m=\u001b[39m pyvips\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mnew_from_file(image_path, access\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msequential\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m     memory_buffer \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_to_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Convert the memory buffer to a NumPy array\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     height, width \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mheight, image\u001b[38;5;241m.\u001b[39mwidth\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyvips/vimage.py:942\u001b[0m, in \u001b[0;36mImage.write_to_memory\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Write the image to a large memory array.\u001b[39;00m\n\u001b[1;32m    923\u001b[0m \n\u001b[1;32m    924\u001b[0m \u001b[38;5;124;03mA large area of memory is allocated, the image is rendered to that\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    939\u001b[0m \n\u001b[1;32m    940\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    941\u001b[0m psize \u001b[38;5;241m=\u001b[39m ffi\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize_t *\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 942\u001b[0m pointer \u001b[38;5;241m=\u001b[39m \u001b[43mvips_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvips_image_write_to_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpointer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpsize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pointer \u001b[38;5;241m==\u001b[39m ffi\u001b[38;5;241m.\u001b[39mNULL:\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Error(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munable to write to memory\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filenames = []\n",
    "image_filenames = generate_image_filenames(imageData['angPos'], imageData['projPerAng'])\n",
    "batch_size = 16\n",
    "\n",
    "if 'props' in locals():\n",
    "    del props\n",
    "props = {}\n",
    "n = len(image_filenames)\n",
    "for i in tqdm(range(0, n, batch_size), total=n // batch_size, desc=\"Processing batches\"):\n",
    "    batch_filenames = image_filenames[i:i + batch_size]\n",
    "    batch_image_paths = [os.path.join(imageData['path'], filename) for filename in batch_filenames]\n",
    "\n",
    "    # Load the batch\n",
    "    batch_images = load_batch(batch_image_paths)\n",
    "\n",
    "    # Preprocess the batch\n",
    "    preprocessed_batch_images = preprocess_images(batch_images)\n",
    "\n",
    "    # Predict images\n",
    "    predictions = model.predict_on_batch(preprocessed_batch_images)\n",
    "    \n",
    "    \n",
    "    processed_images = post_processing(predictions)\n",
    "\n",
    "    get_regionprops(processed_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVPIeYDYZQNO"
   },
   "outputs": [],
   "source": [
    "#TEST\n",
    "df = pd.DataFrame(props)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1687203088414,
     "user": {
      "displayName": "Philipp W.",
      "userId": "16506019178879371982"
     },
     "user_tz": -120
    },
    "id": "sOpKyP5oEJdx",
    "outputId": "da1ea37c-31c0-47ef-8b50-c385809a86e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8298.761538461538\n"
     ]
    }
   ],
   "source": [
    "#Add filenames to dictionary\n",
    "pattern = r\"Scan\\d+_(\\d+)_(\\d+)\\.png\"\n",
    "\n",
    "new_props = {key: {**props[key], 'filename': filename, 'AngPos': int(re.search(pattern, filename).group(1)), 'ProjperAng': int(re.search(pattern, filename).group(2))} for key, filename in zip(props.keys(), filenames)}\n",
    "\n",
    "#Calculate average\n",
    "total_area = 0\n",
    "num_entries = len(props)\n",
    "\n",
    "for values in new_props.values():\n",
    "    area = values['area']\n",
    "    total_area += area\n",
    "\n",
    "# Calculate the average height\n",
    "average_area = total_area / num_entries\n",
    "print(average_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 521,
     "status": "ok",
     "timestamp": 1687203533891,
     "user": {
      "displayName": "Philipp W.",
      "userId": "16506019178879371982"
     },
     "user_tz": -120
    },
    "id": "_ojqnte1sRo2",
    "outputId": "c8358648-3a15-465f-aa35-cb76281e43fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[708, 1035, 1147, 1608, 1657, 2037, 2078, 2259, 2406, 2521, 2715, 2746, 2773, 3020, 3041, 3189, 3370, 3502, 3533, 3759, 3839, 3839, 4048, 4080, 4087, 4259, 4305, 4369, 4522, 4549, 4716, 4773, 4784, 4906, 4972, 4987, 5211, 5224, 5364, 5552, 5590, 5690, 5859, 5933, 5976, 6160, 6242, 6291, 6326, 6464, 6475, 6559, 6727, 6887, 6901, 7034, 7084, 7213, 7253, 7475, 7512, 7540, 7652, 7796, 7922, 8028, 8083, 8152, 8279, 8344, 8347, 8427, 8555, 8712, 8855, 8966, 9023, 9173, 9292, 9487, 9516, 9529, 9801, 9903, 9904, 10080, 10151, 10282, 10520, 10569, 10701, 10795, 11013, 11105, 11166, 11360, 11498, 11520, 11726, 11867, 11963, 12138, 12173, 12278, 12400, 12772, 12777, 12814, 13153, 13175, 13190, 13413, 13570, 13834, 14038, 14159, 14371, 14598, 14608, 14652, 15174, 15285, 15351, 15643, 15832, 16063, 16448, 16679, 16691, 16817]\n",
      "[708.0, 3929.8, 7151.6, 10373.400000000001, 13595.2, 16817.0]\n",
      "Step 1 filenames: ['Scan070602_00000000_0041.png', 'Scan070602_00000001_0032.png']\n",
      "Step 2 filenames: ['Scan070602_00000000_0005.png', 'Scan070602_00000001_0040.png']\n",
      "Step 3 filenames: ['Scan070602_00000000_0017.png', 'Scan070602_00000001_0049.png']\n",
      "Step 4 filenames: ['Scan070602_00000000_0026.png', 'Scan070602_00000001_0057.png']\n",
      "Step 5 filenames: ['Scan070602_00000000_0034.png', 'Scan070602_00000001_0020.png']\n",
      "Step 6 filenames: ['Scan070602_00000000_0039.png', 'Scan070602_00000001_0030.png']\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "# Extract the areas from the bounding boxes\n",
    "areas = [area for values in new_props.values() for area in [values['area']]]\n",
    "\n",
    "\n",
    "# Calculate the lower and upper quartiles\n",
    "q1 = statistics.quantiles(areas, n=4)[0]\n",
    "q3 = statistics.quantiles(areas, n=4)[-1]\n",
    "\n",
    "# Calculate the interquartile range (IQR)\n",
    "iqr = q3 - q1\n",
    "\n",
    "# Calculate the lower and upper bounds for outliers\n",
    "lower_bound = q1 - (0.8 * iqr)\n",
    "upper_bound = q3 + (0.8 * iqr)\n",
    "\n",
    "# Remove outliers from the areas list\n",
    "filtered_areas = [area for area in areas if lower_bound <= area <= upper_bound]\n",
    "print(sorted(filtered_areas))\n",
    "# Find the minimum and maximum areas from the filtered areas list\n",
    "minimum_area = min(filtered_areas)\n",
    "maximum_area = max(filtered_areas)\n",
    "interval = (maximum_area - minimum_area) / 5\n",
    "steps = [minimum_area + interval * i for i in range(6)]\n",
    "#steps = [60, 65, 70, 75, 80, 85]\n",
    "print(steps)\n",
    "# Find the closest fitting bounding box for each unique AngPos\n",
    "\n",
    "step_filenames = []\n",
    "\n",
    "# Specify the parent directory to create the step folders\n",
    "parent_directory = imageData['path']\n",
    "\n",
    "# Create the parent directory if it doesn't exist\n",
    "if not os.path.exists(parent_directory):\n",
    "    os.makedirs(parent_directory)\n",
    "\n",
    "for i, step in enumerate(steps):\n",
    "    ang_positions = set(entry['AngPos'] for entry in new_props.values())\n",
    "\n",
    "    # Initialize a list to store filenames for the current step\n",
    "    current_step_filenames = []\n",
    "\n",
    "    # Create the step directory\n",
    "    step_directory = os.path.join(parent_directory, f'Step{i+1}')\n",
    "    os.makedirs(step_directory)\n",
    "\n",
    "    for ang_position in sorted(ang_positions):\n",
    "        # Select the entries with the current 'AngPos' value\n",
    "        selected_entries = [entry for entry in new_props.values() if entry['AngPos'] == ang_position]\n",
    "\n",
    "        # Find the closest fitting bounding box for the current step and 'AngPos'\n",
    "        closest_fitting_area = None\n",
    "        min_distance = float('inf')\n",
    "\n",
    "        for entry in selected_entries:\n",
    "            area = entry['area']  # Calculate the height\n",
    "            distance = abs(area - step)\n",
    "\n",
    "            if distance < min_distance:\n",
    "                closest_fitting_area = entry\n",
    "                min_distance = distance\n",
    "\n",
    "        # Append the filename for the closest fitting bounding box to the current step filenames list\n",
    "        if closest_fitting_area is not None:\n",
    "            filename = closest_fitting_area['filename']\n",
    "            current_step_filenames.append(filename)\n",
    "\n",
    "            # Move the file to the step directory\n",
    "            source_path = os.path.join(imageData['path'], filename)\n",
    "            destination_path = os.path.join(step_directory, filename)\n",
    "            shutil.copy(source_path, destination_path)\n",
    "\n",
    "    # Append the current step filenames to the overall list\n",
    "    step_filenames.append(current_step_filenames)\n",
    "\n",
    "# Print the step filenames\n",
    "for i, filenames in enumerate(step_filenames):\n",
    "    print(f\"Step {i+1} filenames: {filenames}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOl98u1oTSuV9CO4o1FU/Uz",
   "gpuType": "T4",
   "mount_file_id": "1Qu-T0eVaXhhCcKIiOzJ4sq2jEjPcQdbb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
